# 누끼 따기 (Background Removal) 기술 문서

## 1. 개요

무신사에서 크롤링한 상품 이미지의 배경을 제거하여 코디 조합 UI에 활용하기 위한 기술 문서입니다.

---

## 2. 기술 스택 선택

### 2.1 추천: rembg (Python)

```
rembg - AI 기반 배경 제거 라이브러리
```

| 항목 | 내용 |
|------|------|
| 라이브러리 | `rembg` |
| AI 모델 | U2-Net (사전 훈련된 딥러닝 모델) |
| 장점 | 설치 간편, 높은 정확도, 의류 이미지에 최적화 |
| 단점 | 첫 실행 시 모델 다운로드 필요 (~170MB) |

### 2.2 대안 옵션

| 옵션 | 장점 | 단점 |
|------|------|------|
| **remove.bg API** | 최고 품질, 간편 | 유료 (월 50회 무료) |
| **OpenCV + GrabCut** | 무료, 가벼움 | 품질 낮음, 수동 조정 필요 |
| **Segment Anything (SAM)** | 최신 기술, 고품질 | 무거움, GPU 필요 |

---

## 3. 구현 계획

### 3.1 시스템 아키텍처

```
┌─────────────────────────────────────────────────────────────────┐
│                        Frontend (React)                         │
│  - 코디 추천 결과 표시                                            │
│  - 누끼 이미지 조합 렌더링                                         │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                      Backend API (FastAPI)                      │
│  - /api/recommend : AI 코디 추천                                 │
│  - /api/products : 상품 정보 조회                                │
│  - /api/process-image : 누끼 처리 요청                           │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    Image Processing Service                     │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐         │
│  │  Scraper    │───▶│   rembg     │───▶│   Storage   │         │
│  │ (무신사)    │    │ (누끼처리)  │    │  (S3/Local) │         │
│  └─────────────┘    └─────────────┘    └─────────────┘         │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 처리 플로우

```
1. 크롤링 요청
   └─▶ 무신사 상품 페이지 스크래핑
       └─▶ 상품 이미지 URL 추출

2. 이미지 다운로드
   └─▶ 원본 이미지 저장 (temp/)

3. 누끼 처리
   └─▶ rembg로 배경 제거
       └─▶ PNG 형식으로 저장 (투명 배경)

4. 저장 및 캐싱
   └─▶ 처리된 이미지 저장
       └─▶ 상품 ID와 매핑하여 DB 저장

5. 프론트엔드 제공
   └─▶ 누끼 이미지 URL 반환
```

---

## 4. 코드 구현

### 4.1 환경 설정

```bash
# 가상환경 생성
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 패키지 설치
pip install rembg pillow requests beautifulsoup4 fastapi uvicorn
```

### 4.2 requirements.txt

```txt
rembg==2.0.50
pillow>=10.0.0
requests>=2.31.0
beautifulsoup4>=4.12.0
fastapi>=0.104.0
uvicorn>=0.24.0
aiofiles>=23.2.0
python-multipart>=0.0.6
```

### 4.3 누끼 처리 모듈

```python
# services/image_processor.py

from rembg import remove
from PIL import Image
import io
import requests
from pathlib import Path

class ImageProcessor:
    def __init__(self, output_dir: str = "processed_images"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def download_image(self, url: str) -> Image.Image:
        """URL에서 이미지 다운로드"""
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return Image.open(io.BytesIO(response.content))

    def remove_background(self, image: Image.Image) -> Image.Image:
        """rembg를 사용하여 배경 제거"""
        # RGBA로 변환 (투명 배경 지원)
        if image.mode != "RGBA":
            image = image.convert("RGBA")

        # 배경 제거
        output = remove(image)
        return output

    def process_product_image(self, url: str, product_id: str) -> str:
        """상품 이미지 전체 처리 파이프라인"""
        # 1. 이미지 다운로드
        image = self.download_image(url)

        # 2. 배경 제거
        processed = self.remove_background(image)

        # 3. 저장
        output_path = self.output_dir / f"{product_id}.png"
        processed.save(output_path, "PNG")

        return str(output_path)

    def process_batch(self, items: list[dict]) -> list[dict]:
        """여러 상품 일괄 처리"""
        results = []
        for item in items:
            try:
                path = self.process_product_image(
                    url=item["image_url"],
                    product_id=item["product_id"]
                )
                results.append({
                    "product_id": item["product_id"],
                    "processed_path": path,
                    "status": "success"
                })
            except Exception as e:
                results.append({
                    "product_id": item["product_id"],
                    "error": str(e),
                    "status": "failed"
                })
        return results
```

### 4.4 무신사 크롤러

```python
# services/musinsa_scraper.py

import requests
from bs4 import BeautifulSoup
from dataclasses import dataclass
from typing import Optional

@dataclass
class Product:
    product_id: str
    name: str
    brand: str
    price: int
    image_url: str
    category: str
    link: str

class MusinsaScraper:
    BASE_URL = "https://www.musinsa.com"

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })

    def search_products(
        self,
        keyword: str,
        category: Optional[str] = None,
        min_price: int = 0,
        max_price: int = 500000,
        limit: int = 10
    ) -> list[Product]:
        """무신사 상품 검색"""
        # 검색 API 또는 페이지 스크래핑
        search_url = f"{self.BASE_URL}/search/musinsa/goods"
        params = {
            "q": keyword,
            "min_price": min_price,
            "max_price": max_price,
        }

        response = self.session.get(search_url, params=params)
        soup = BeautifulSoup(response.text, "html.parser")

        products = []
        items = soup.select(".search-list .list-item")[:limit]

        for item in items:
            try:
                product = self._parse_product_item(item)
                if product:
                    products.append(product)
            except Exception:
                continue

        return products

    def _parse_product_item(self, item) -> Optional[Product]:
        """상품 아이템 파싱"""
        # 실제 무신사 HTML 구조에 맞게 조정 필요
        product_id = item.get("data-goods-no", "")
        name = item.select_one(".article_info .title")
        brand = item.select_one(".article_info .brand")
        price = item.select_one(".article_info .price")
        image = item.select_one("img")
        link = item.select_one("a")

        if not all([product_id, name, image]):
            return None

        return Product(
            product_id=product_id,
            name=name.text.strip(),
            brand=brand.text.strip() if brand else "",
            price=self._parse_price(price.text if price else "0"),
            image_url=image.get("src", ""),
            category="",
            link=f"{self.BASE_URL}{link.get('href', '')}" if link else ""
        )

    def _parse_price(self, price_str: str) -> int:
        """가격 문자열 파싱"""
        return int("".join(filter(str.isdigit, price_str)) or 0)

    def get_product_detail(self, product_id: str) -> Optional[Product]:
        """상품 상세 정보 조회"""
        url = f"{self.BASE_URL}/app/goods/{product_id}"
        response = self.session.get(url)
        soup = BeautifulSoup(response.text, "html.parser")

        # 상세 페이지 파싱 로직
        # ...

        return None
```

### 4.5 FastAPI 서버

```python
# main.py

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional
import uvicorn

from services.image_processor import ImageProcessor
from services.musinsa_scraper import MusinsaScraper

app = FastAPI(title="뭐입지 API", version="1.0.0")

# CORS 설정
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 정적 파일 서빙 (처리된 이미지)
app.mount("/images", StaticFiles(directory="processed_images"), name="images")

# 서비스 인스턴스
image_processor = ImageProcessor()
scraper = MusinsaScraper()


class RecommendRequest(BaseModel):
    occasion: str      # 어디 가요?
    style: str         # 어떻게 보이고 싶어요?
    body_concern: str  # 체형 고민?
    budget: int        # 예산


class ProcessImageRequest(BaseModel):
    image_url: str
    product_id: str


@app.post("/api/recommend")
async def get_recommendation(request: RecommendRequest):
    """AI 코디 추천 API"""
    # TODO: AI 모델 연동
    # 현재는 Mock 데이터 반환

    # 1. 무신사에서 조건에 맞는 상품 검색
    tops = scraper.search_products("상의", max_price=request.budget // 3, limit=5)
    bottoms = scraper.search_products("하의", max_price=request.budget // 3, limit=5)
    shoes = scraper.search_products("신발", max_price=request.budget // 3, limit=5)

    # 2. AI 추천 로직 (추후 구현)
    recommendation = {
        "top": tops[0] if tops else None,
        "bottom": bottoms[0] if bottoms else None,
        "shoes": shoes[0] if shoes else None,
    }

    return {
        "success": True,
        "recommendation": recommendation,
        "total_price": sum([
            item.price for item in recommendation.values() if item
        ])
    }


@app.post("/api/process-image")
async def process_image(request: ProcessImageRequest, background_tasks: BackgroundTasks):
    """이미지 배경 제거 API"""
    try:
        # 백그라운드에서 처리
        output_path = image_processor.process_product_image(
            url=request.image_url,
            product_id=request.product_id
        )

        return {
            "success": True,
            "processed_url": f"/images/{request.product_id}.png"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/products/search")
async def search_products(
    keyword: str,
    min_price: int = 0,
    max_price: int = 500000,
    limit: int = 10
):
    """상품 검색 API"""
    products = scraper.search_products(
        keyword=keyword,
        min_price=min_price,
        max_price=max_price,
        limit=limit
    )

    return {
        "success": True,
        "count": len(products),
        "products": [p.__dict__ for p in products]
    }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 5. 최적화 전략

### 5.1 캐싱

```python
# services/cache.py

import hashlib
from pathlib import Path
from typing import Optional

class ImageCache:
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def get_cache_key(self, url: str) -> str:
        """URL을 해시하여 캐시 키 생성"""
        return hashlib.md5(url.encode()).hexdigest()

    def get(self, url: str) -> Optional[str]:
        """캐시에서 이미지 경로 조회"""
        key = self.get_cache_key(url)
        cache_path = self.cache_dir / f"{key}.png"

        if cache_path.exists():
            return str(cache_path)
        return None

    def set(self, url: str, image_path: str) -> None:
        """캐시에 이미지 저장"""
        key = self.get_cache_key(url)
        # 이미지를 캐시 디렉토리로 복사
        # ...
```

### 5.2 비동기 처리

```python
# services/async_processor.py

import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncImageProcessor:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.processor = ImageProcessor()

    async def process_async(self, url: str, product_id: str) -> str:
        """비동기 이미지 처리"""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor,
            self.processor.process_product_image,
            url,
            product_id
        )
        return result

    async def process_batch_async(self, items: list[dict]) -> list[dict]:
        """여러 이미지 동시 처리"""
        tasks = [
            self.process_async(item["image_url"], item["product_id"])
            for item in items
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
```

---

## 6. 디렉토리 구조

```
backend/
├── main.py                    # FastAPI 앱 진입점
├── requirements.txt           # 의존성
├── services/
│   ├── __init__.py
│   ├── image_processor.py     # 누끼 처리
│   ├── musinsa_scraper.py     # 무신사 크롤링
│   ├── cache.py               # 캐싱
│   └── async_processor.py     # 비동기 처리
├── models/
│   ├── __init__.py
│   └── schemas.py             # Pydantic 모델
├── processed_images/          # 처리된 이미지 저장
├── cache/                     # 캐시 디렉토리
└── tests/
    └── test_processor.py
```

---

## 7. 주의사항

### 7.1 무신사 크롤링 관련

- robots.txt 준수
- 요청 간 적절한 딜레이 (1-2초)
- User-Agent 헤더 설정
- 과도한 요청 시 IP 차단 가능성

### 7.2 rembg 관련

- 첫 실행 시 모델 다운로드 (약 170MB)
- GPU 사용 시 성능 향상 (CUDA 지원)
- 메모리 사용량: 이미지당 약 500MB~1GB

### 7.3 이미지 처리

- 고해상도 이미지는 리사이즈 후 처리 권장
- PNG 형식으로 저장 (투명 배경)
- 원본 이미지 백업 권장

---

## 8. 테스트

```python
# tests/test_processor.py

import pytest
from services.image_processor import ImageProcessor

def test_remove_background():
    processor = ImageProcessor()

    # 테스트 이미지 URL
    test_url = "https://image.musinsa.com/images/goods_img/..."

    result = processor.process_product_image(test_url, "test_001")

    assert result.endswith(".png")
    assert Path(result).exists()
```

```bash
# 테스트 실행
pytest tests/ -v
```

---

## 9. 배포 고려사항

### 9.1 Docker

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# 시스템 의존성
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 9.2 환경 변수

```env
# .env
MUSINSA_DELAY=1.5
CACHE_TTL=86400
MAX_WORKERS=4
OUTPUT_DIR=processed_images
```
